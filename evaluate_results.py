''''
Information retrieval evaluation script: 

Evaluate results of an information retrieval system which outputs trec-formatted tsv files. 
'''


import argparse
import os
import pytrec_eval
import pandas as pd
import matplotlib.pyplot as plt

def load_qrels(qrels_file):
    """
    Load qrels from a file.

    Args:
        qrels_file (str): Path to the qrels file.

    Returns:
        dict: A dictionary containing the qrels.
    """
    qrels = {}
    with open(qrels_file, 'r') as file:
        for line in file:
            qid, _, docid, rel = line.strip().split()
            if qid not in qrels:
                qrels[qid] = {}
            qrels[qid][docid] = int(rel)
    return qrels

def load_results(results_file):
    """
    Load results from the tsv file generated by the boolean retrieval system. 
    Results will consist of top 100 documents for each query.

    Args:
        results_file (str): Path to the results file.

    Returns:
        dict: A dictionary containing the results.
    """
    results = {}
    with open(results_file, 'r') as file:
        for line in file:
            qid, _, docid, rank, score, _ = line.strip().split()
            if qid not in results:
                results[qid] = {}
            results[qid][docid] = float(score)
    return results

def evaluate_results(qrels, results):
    """
    Evaluate the results using pytrec_eval.

    Args:
        qrels (dict): A dictionary containing the qrels.
        results (dict): A dictionary containing the results.

    Returns:
        dict: A dictionary containing the aggregated evaluation metrics.
        pd.DataFrame: A DataFrame containing the full evaluation metrics for each query.
    """
    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'P.1', 'P.5', 'P.10', 'ndcg_cut.5', 'recip_rank', 'map'})
    metrics = evaluator.evaluate(results)
    
    aggregated_metrics = {metric: 0.0 for metric in metrics[next(iter(metrics))].keys()}
    
    for qid in metrics:
        for metric in metrics[qid]:
            aggregated_metrics[metric] += metrics[qid][metric]
    
    for metric in aggregated_metrics:
        aggregated_metrics[metric] /= len(metrics)
    
    metrics_df = pd.DataFrame.from_dict(metrics, orient='index')
    
    return aggregated_metrics, metrics_df

def plot_precision(full_metrics, precision_level, output_file):
    """
    Plot precision at the specified level using descending bars.

    Args:
        full_metrics (pd.DataFrame): DataFrame containing the full evaluation metrics for each query.
        precision_level (str): The precision level to plot (e.g., 'P_1', 'P_5', 'P_10').
        output_file (str): Path to the output file for saving the plot.
    """
    # Sort the DataFrame by the specified precision level in descending order
    sorted_metrics = full_metrics.sort_values(by=precision_level, ascending=False)
    # Select the top 100 queries
    top_100_metrics = sorted_metrics.head(100)
    # Get the sorted precision values
    precision_values = top_100_metrics[precision_level].values
    # Get the sorted query IDs
    query_ids = top_100_metrics.index
    # Create a figure and axis with a larger size
    fig, ax = plt.subplots(figsize=(15, 10))
    # Create a vertical bar plot
    ax.bar(query_ids, precision_values, color='skyblue')
    # Set the title and labels
    ax.set_title(f'Precision@{precision_level.split("_")[1]} for Top 100 Queries')
    ax.set_xlabel('Query ID')
    ax.set_ylabel(f'Precision@{precision_level.split("_")[1]}')
    # Rotate x-axis labels for better readability
    plt.xticks(rotation=90)
    # Save the plot
    plt.savefig(output_file)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the results of the boolean retrieval system using pytrec_eval.")
    parser.add_argument('--results', '-r', type=str, required=True, help="Path to the results file in TREC format.")
    parser.add_argument('--qrels', '-q', type=str, required=True, help="Path to the qrels file (ground truth relevance judgments).")
    parser.add_argument('--output', '-o', type=str, required=True, help="Path to the output file for saving metrics.")
    args = parser.parse_args()
    
    qrels_file = args.qrels
    results_file = args.results
    out_metrics_file = args.output

    script_dir = os.path.dirname(os.path.abspath(__file__))
    os.chdir(script_dir)

    qrels = load_qrels(qrels_file)
    results = load_results(results_file)

    aggregated_metrics, full_metrics = evaluate_results(qrels, results)

    print("Aggregated Metrics:")
    for metric, value in aggregated_metrics.items():
        print(f"{metric}: {value:.4f}")

    with open(out_metrics_file, 'w') as output_file:
        output_file.write("Aggregated Metrics:\n")
        for metric, value in aggregated_metrics.items():
            output_file.write(f"{metric}: {value:.4f}\n")

    # Dir for saving the plots
    output_dir = os.path.join(script_dir, 'figs')
    os.makedirs(output_dir, exist_ok=True)

    # Plot and save precision plots for P_1, P_5, and P_10
    for precision_level in ['P_1', 'P_5', 'P_10']:
        plot_file = os.path.join(output_dir, f'{precision_level.lower()}_plot.png')
        plot_precision(full_metrics, precision_level, plot_file)